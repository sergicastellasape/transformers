{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import umap\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom.transformer_sentence import TransformerSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerSentence():\n",
    "    def __init__(self, sentence_str, \n",
    "                 model=BertModel.from_pretrained('scibert-scivocab-uncased'), \n",
    "                 tokenizer=BertTokenizer.from_pretrained('scibert-scivocab-uncased')):\n",
    "        \n",
    "        self.raw_string = sentence_str\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.summary = {}\n",
    "\n",
    "        \n",
    "    def write_summary(self, input_tokens=None, \n",
    "                      hidden_states=None, \n",
    "                      hidden_attentions=None,\n",
    "                      print_tokens=True):\n",
    "        \n",
    "        if (input_tokens or hidden_states or hidden_attentions) is None:\n",
    "            input_tokens, hidden_states, hidden_attentions = self.forward()\n",
    "        \n",
    "        # this replaces adds a \"_{counter}\" to the repreated tokens, so that \n",
    "        # they can be used uniquely as the keys for the embeddings dictionary\n",
    "        input_tokens = TransformerSentence.make_unique(input_tokens)\n",
    "        \n",
    "        if print_tokens:\n",
    "            print('Sentence Tokenization: ', input_tokens)\n",
    "            \n",
    "        # write summary into the object\n",
    "        self.summary['input_tokens'] = input_tokens\n",
    "        self.summary['states'] = hidden_states\n",
    "        self.summary['attentions'] = hidden_attentions\n",
    "\n",
    "        self.summary['token_embeddings'] = {input_token: hidden_states[:, i, :] \n",
    "                                            for i, input_token in enumerate(input_tokens)}\n",
    "        \n",
    "    def forward(self):\n",
    "        encoded_inputs_dict = self.tokenizer.encode_plus(self.raw_string)\n",
    "        input_ids = encoded_inputs_dict['input_ids']\n",
    "        input_tensor = torch.tensor([input_ids])\n",
    "        input_tokens = [self.tokenizer.decode(input_ids[j]).replace(' ', '') \n",
    "                        for j in range(len(input_ids))]\n",
    "        \n",
    "        final_attention, final_state, hidden_states_tup, hidden_attentions_tup = self.model(input_tensor)\n",
    "        \n",
    "        # stacking states and attentions along the first dimention (which corresponds to the batch when necessary)\n",
    "        hidden_attentions = torch.cat(hidden_attentions_tup, dim=0) # 'layers', 'heads', 'queries', 'keys'\n",
    "        hidden_states = torch.cat(hidden_states_tup, dim=0) # 'layers', 'tokens', 'embeddings'\n",
    "        \n",
    "        return input_tokens, hidden_states.detach(), hidden_attentions.detach()\n",
    "    \n",
    "    \n",
    "    def attention_from_tokens(self, token1, token2, display=True):\n",
    "        input_tokens = self.summary['input_tokens']\n",
    "        \n",
    "        if (token1 and token2) not in input_tokens:\n",
    "            raise ValueError('One or both of the tokens introduced are not in the sentence!')\n",
    "            \n",
    "        idx1, idx2 = input_tokens.index(token1), input_tokens.index(token2)\n",
    "        attention = self.summary['attentions'][:, :, idx1, idx2].numpy()\n",
    "        if display:\n",
    "            TransformerSentence.display_attention(attention, title=(token1, token2))\n",
    "        return attention\n",
    "    \n",
    "    \n",
    "    def attention_from_idx(self, i, j, display=True):\n",
    "        attention = self.summary['attentions'][:, :, i, j].numpy()\n",
    "        if display:\n",
    "            TransformerSentence.display_attention(attention, title=f'Token idx: {(i, j)}')\n",
    "        return attention\n",
    "    \n",
    "    def visualize_token_path(self, fit, \n",
    "                             tokens_to_follow=None, \n",
    "                             print_tokens=False, \n",
    "                             fig_axs=(None, None), \n",
    "                             figsize=(10, 10)):\n",
    "        \n",
    "        if tokens_to_follow is None:\n",
    "            all_tokens = self.summary['input_tokens']\n",
    "            regex = re.compile(r'^[a-zA-Z]')\n",
    "            tokens_to_follow = [i for i in all_tokens if regex.search(i)]\n",
    "            \n",
    "        if print_tokens: print(tokens_to_follow)  \n",
    "            \n",
    "        colors = list(range(len(tokens_to_follow)))\n",
    "        projections = []\n",
    "        layer_depth = self.summary['states'].size()[0]\n",
    "        \n",
    "        for i in range(layer_depth):\n",
    "            layer_embeddings = self.summary['states'][i, :, :]\n",
    "            projection = fit.transform(layer_embeddings)\n",
    "            projections.append(projection)\n",
    "\n",
    "        data = np.stack(projections, axis=0)\n",
    "        if None in fig_axs:\n",
    "            fig, axs = plt.subplots(figsize=figsize)\n",
    "        for token in tokens_to_follow:\n",
    "            i = self.summary['input_tokens'].index(token)\n",
    "            plt.plot(data[:,i,0], data[:,i,1], '-o', alpha=0.3)\n",
    "            plt.annotate(s=token, xy=(data[0, i, 0], data[0, i, 1]))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_sentence_shape(self, fit, tokens_to_follow=None, \n",
    "                                 print_tokens=False, \n",
    "                                 fig_axs=(None, None), \n",
    "                                 figsize=(10, 10)):\n",
    "\n",
    "        if tokens_to_follow is None:\n",
    "            all_tokens = self.summary['input_tokens']\n",
    "            regex = re.compile(r'^[a-zA-Z]')\n",
    "            tokens_to_follow = [i for i in all_tokens if regex.search(i)]\n",
    "\n",
    "        if print_tokens: print(tokens_to_follow)  \n",
    "\n",
    "        colors = list(range(len(tokens_to_follow)))\n",
    "        projections = []\n",
    "        layer_depth = self.summary['states'].size()[0]\n",
    "        \n",
    "        # get list of indeces of the tokens to follow\n",
    "        idxs = [self.summary['input_tokens'].index(token) for token in tokens_to_follow] \n",
    "        token_embeddings = self.summary['states'][-1, idxs, :]\n",
    "        data = fit.transform(token_embeddings)\n",
    "        \n",
    "        if None in fig_axs:\n",
    "            fig, axs = plt.subplots(figsize=figsize)\n",
    "            \n",
    "        plt.plot(data[:,0], data[:,1], '-o')\n",
    "        for i, token in enumerate(tokens_to_follow):\n",
    "            plt.annotate(s=token, xy=(data[i, 0], data[i, 1]))\n",
    "        #plt.show()\n",
    "    \n",
    "    \n",
    "    def save(self, name, path='.'):\n",
    "        with open(os.path.join(path, name), 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_embedding(embedding, title=None, vmax=None, vmin=None):\n",
    "        if (vmax or vmin) is None:\n",
    "            vmax = max(embedding)\n",
    "            vmin = min(embedding)\n",
    "            \n",
    "        N = embedding.size()[0]\n",
    "        h = math.ceil(math.sqrt(N))\n",
    "        # N = a*b where abs(a-b) is minimum\n",
    "        while (N % h != 0):\n",
    "            h -= 1\n",
    "        w = int(N / h)\n",
    "        visualization = embedding.reshape((h, w)).numpy()\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(visualization, vmax=vmax, vmin=vmin, cmap='viridis')\n",
    "        fig.colorbar(im)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def display_attention(attention, title=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(attention, vmin=0., vmax=1., cmap='viridis')\n",
    "        fig.colorbar(im)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xlabel('HEADS')\n",
    "        ax.set_ylabel('LAYERS')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(name, path='.'):\n",
    "        with open(os.path.join(path, name), 'rb') as file:\n",
    "            SentenceObject = pickle.load(file)\n",
    "        return SentenceObject\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_unique(L):\n",
    "        unique_L = []\n",
    "        for i, v in enumerate(L):\n",
    "            totalcount = L.count(v)\n",
    "            count = L[:i].count(v)\n",
    "            unique_L.append(v + '_' + str(count+1) if totalcount > 1 else v)\n",
    "        return unique_L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preloading models (this is the most costly)\n",
    "# Bert base and large, uncased\n",
    "BertBaseModel = BertModel.from_pretrained('bert-base-uncased')\n",
    "BertBaseTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BertLargeModel = BertModel.from_pretrained('bert-large-uncased')\n",
    "BertLargeTokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "# Scibert uncased\n",
    "SciBertModel = BertModel.from_pretrained('scibert-scivocab-uncased')\n",
    "SciBertTokenizer = BertTokenizer.from_pretrained('scibert-scivocab-uncased')\n",
    "SciBertBaseVocabModel = BertModel.from_pretrained('scibert-basevocab-uncased')\n",
    "SciBertBaseVocabTokenizer = BertTokenizer.from_pretrained('scibert-basevocab-uncased')\n",
    "# Scibert Cased\n",
    "#SciBertModelCased = BertModel.from_pretrained('scibert-scivocab-cased')\n",
    "#SciBertTokenizerCased = BertTokenizer.from_pretrained('scibert-scivocab-cased')\n",
    "#SciBertBaseVocabModelCased = BertModel.from_pretrained('scibert-basevocab-cased')\n",
    "#SciBertBaseVocabTokenizerCased = BertTokenizer.from_pretrained('scibert-basevocab-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def load_dataset(txt_path=\"../datasets/quora_questions.txt\", \n",
    "                 MODEL=SciBertModel,\n",
    "                 TOKENIZER=SciBertTokenizer):\n",
    "    \n",
    "    # Read input sequences from .txt file and put them in a list\n",
    "    with open(txt_path) as f:\n",
    "        text = f.read()\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    try:\n",
    "        sentences.remove('') # remove possible empty strings\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    list_SentenceObj, ALL_INITIAL_EMBEDDINGS, ALL_CONTEXT_EMBEDDINGS = [], [], []\n",
    "    \n",
    "    for raw_sentence in tqdm(sentences):\n",
    "        SentenceObj = TransformerSentence(raw_sentence,\n",
    "                                          model=MODEL,\n",
    "                                          tokenizer=TOKENIZER)\n",
    "        SentenceObj.write_summary(print_tokens=False)\n",
    "        list_SentenceObj.append(SentenceObj)\n",
    "        ALL_INITIAL_EMBEDDINGS.append(SentenceObj.summary['states'][0, :, :])\n",
    "        ALL_CONTEXT_EMBEDDINGS.append(SentenceObj.summary['states'][-1, :, :])\n",
    "\n",
    "    ALL_INITIAL_EMBEDDINGS = torch.cat(ALL_INITIAL_EMBEDDINGS, dim=0)\n",
    "    ALL_CONTEXT_EMBEDDINGS = torch.cat(ALL_CONTEXT_EMBEDDINGS, dim=0)\n",
    "    \n",
    "    return list_SentenceObj, ALL_INITIAL_EMBEDDINGS, ALL_CONTEXT_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:  ['[CLS]', 'the', 'conference', 'is', 'in', 'new', 'york', 'and', 'i', 'like', 'it', '.', '[SEP]']\n",
      "Sentence Tokenization:  ['[CLS]', 'london', 'is', 'a', 'beautiful', 'city', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "_ = torch.manual_seed(0)\n",
    "s1 = \"The conference is in New York and I like it.\"\n",
    "s2 = \"London is a beautiful city.\"\n",
    "sentence1 = TransformerSentence(s1, model=BertBaseModel, tokenizer=BertBaseTokenizer)\n",
    "sentence2 = TransformerSentence(s2, model=BertBaseModel, tokenizer=BertBaseTokenizer)\n",
    "\n",
    "sentence1.write_summary(print_tokens=True)\n",
    "sentence2.write_summary(print_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Distance Based Compacting Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     2,
     6,
     47
    ]
   },
   "outputs": [],
   "source": [
    "cos = torch.nn.CosineSimilarity(dim=1, eps=1e-5) # similarity func.\n",
    "\n",
    "def remove_subsets(L):\n",
    "    filtered = filter(lambda f: not any(set(f) < set(g) for g in L), L)\n",
    "    return list(filtered)\n",
    "\n",
    "def indices_to_compact_by_similarity_threshold(sequence_embeddings,\n",
    "                                               sim_function=cos,\n",
    "                                               threshold=0.1,\n",
    "                                               exclude_special_tokens=True,\n",
    "                                               combinatorics=None):\n",
    "    # combinatorics= 'sequential', 'all'\n",
    "    seq_length, embedding_size = sequence_embeddings.size() #make sure the input is proper size!!\n",
    "    indices = list(range(seq_length))    \n",
    "    \n",
    "    # Combinations of indices that are group candidates\n",
    "    if combinatorics == 'sequential':\n",
    "        if exclude_special_tokens:\n",
    "            idx_combinations = [indices[s:e] for s, e in itertools.combinations(range(1, len(indices)), 2)]\n",
    "        else:\n",
    "            idx_combinations = [indices[s:e] for s, e in itertools.combinations(range(len(indices)+1), 2)]\n",
    "            \n",
    "    elif combinatorics == 'all':\n",
    "        idx_combinations = []\n",
    "        for L in range(2, seq_length+1):\n",
    "            combinations = list(itertools.combinations(indices, r=L))\n",
    "            idx_combinations.extend(combinations)\n",
    "    else:\n",
    "        raise ValueError('You must specify the combinatorics as \"sequencial\" or \"all\"!!')\n",
    "    \n",
    "    \n",
    "    all_indices_to_compact = []\n",
    "    for indices in idx_combinations:\n",
    "        group_candidate = sequence_embeddings[indices, :]\n",
    "        group_size = len(indices)\n",
    "        center = torch.mean(group_candidate, dim=0)\n",
    "        center = center.repeat(group_size, 1)\n",
    "        # calculate all embeddings similarities w.r.t. the center of the group\n",
    "        similarities = sim_function(center, group_candidate)\n",
    "        worst_sim, _ = torch.min(similarities, dim=0)\n",
    "        if worst_sim > threshold: all_indices_to_compact.append(indices)\n",
    "            \n",
    "    indices_to_compact = remove_subsets(all_indices_to_compact)\n",
    "    \n",
    "    return indices_to_compact\n",
    "\n",
    "\n",
    "def compact_embeddings(original_embeddings, indices_to_compact):\n",
    "    new_embeddings_list = []\n",
    "    for indices in indices_to_compact:\n",
    "        group = original_embeddings[indices, :]\n",
    "        center = torch.mean(group, dim=0)\n",
    "        new_embeddings_list.append(center)\n",
    "        \n",
    "    new_embeddings = torch.stack(new_embeddings_list, dim=0)\n",
    "    \n",
    "    return new_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Tokenization:  ['[CLS]', 'be', 'sure', 'to', 'put', 'on', 'a', 'life', 'jacket', 'before', 'getting', 'into', 'the', 'boat', '.', '[SEP]']\n",
      "[[1], [2, 3], [3, 4], [5, 6], [7], [8], [9], [10], [11, 12], [13], [14]]\n",
      "Original Length:  16 Compact size:  11\n"
     ]
    }
   ],
   "source": [
    "#s = \"Their car broke down two miles out of town.\"\n",
    "s = \"Be sure to put on a life jacket before getting into the boat.\"\n",
    "#s = \"Itâ€™s time to get on the plane.\"\n",
    "#s = \"what is a bayesian network.\"\n",
    "sentence = TransformerSentence(s, model=BertBaseModel, tokenizer=BertBaseTokenizer)\n",
    "sentence.write_summary()\n",
    "sequence_embeddings = sentence.summary['states'][-1, :, :]\n",
    "indices_to_compact = indices_to_compact_by_similarity_threshold(sequence_embeddings, \n",
    "                                                                sim_function=cos, \n",
    "                                                                threshold=0.90,\n",
    "                                                                exclude_special_tokens=True,\n",
    "                                                                combinatorics='sequential')\n",
    "print(indices_to_compact)\n",
    "new_embeddings = compact_embeddings(sequence_embeddings, indices_to_compact)\n",
    "print('Original Length: ', sequence_embeddings.size()[0], 'Compact size: ', new_embeddings.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[a life jacket, the boat]\n",
      "a life jacket\n",
      "the boat\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-5ad1679ffb89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnoun_chunks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "#!python -m spacy download en\n",
    "\n",
    "nlp = spacy.load(\"en\") # en_core_web_sm\n",
    "\n",
    "doc = nlp(\"Be sure to put on a life jacket before getting into the boat.\")\n",
    "print(list(doc.noun_chunks))\n",
    "for np in doc.noun_chunks: # use np instead of np.text\n",
    "    print(np)\n",
    "\n",
    "print()\n",
    "\n",
    "for sentence in sentences:\n",
    "    doc = nlp(sentence)\n",
    "    noun_chunks.append(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 4, 6, 8]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zeta Alpha",
   "language": "python",
   "name": "za_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
