{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from transformers import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import umap\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TransformerSentence class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerSentence():\n",
    "    def __init__(self, sentence_str,\n",
    "                 model=BertModel.from_pretrained('scibert-scivocab-uncased'), \n",
    "                 tokenizer=BertTokenizer.from_pretrained('scibert-scivocab-uncased')):\n",
    "        \n",
    "        self.raw_string = sentence_str\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.summary = {}\n",
    "\n",
    "        \n",
    "    def write_summary(self, input_tokens=None, \n",
    "                      hidden_states=None, \n",
    "                      hidden_attentions=None,\n",
    "                      print_tokens=True):\n",
    "        \n",
    "        if (input_tokens or hidden_states or hidden_attentions) is None:\n",
    "            input_tokens, hidden_states, hidden_attentions = self.forward()\n",
    "        \n",
    "        # this replaces adds a \"_{counter}\" to the repreated tokens, so that \n",
    "        # they can be used uniquely as the keys for the embeddings dictionary\n",
    "        input_tokens = TransformerSentence.make_unique(input_tokens)\n",
    "        \n",
    "        if print_tokens:\n",
    "            print('Sentence Tokenization: ', input_tokens)\n",
    "            \n",
    "        # write summary into the object\n",
    "        self.summary['input_tokens'] = input_tokens\n",
    "        self.summary['states'] = hidden_states\n",
    "        self.summary['attentions'] = hidden_attentions\n",
    "\n",
    "        self.summary['token_embeddings'] = {input_token: hidden_states[:, i, :] \n",
    "                                            for i, input_token in enumerate(input_tokens)}\n",
    "        \n",
    "    def forward(self):\n",
    "        encoded_inputs_dict = self.tokenizer.encode_plus(self.raw_string)\n",
    "        input_ids = encoded_inputs_dict['input_ids']\n",
    "        input_tensor = torch.tensor([input_ids])\n",
    "        input_tokens = [self.tokenizer.decode(input_ids[j]).replace(' ', '') \n",
    "                        for j in range(len(input_ids))]\n",
    "        \n",
    "        final_attention, final_state, hidden_states_tup, hidden_attentions_tup = self.model(input_tensor)\n",
    "        \n",
    "        # stacking states and attentions along the first dimention (which corresponds to the batch when necessary)\n",
    "        hidden_attentions = torch.cat(hidden_attentions_tup, dim=0) # 'layers', 'heads', 'queries', 'keys'\n",
    "        hidden_states = torch.cat(hidden_states_tup, dim=0) # 'layers', 'tokens', 'embeddings'\n",
    "        \n",
    "        return input_tokens, hidden_states.detach(), hidden_attentions.detach()\n",
    "    \n",
    "    \n",
    "    def attention_from_tokens(self, token1, token2, display=True):\n",
    "        input_tokens = self.summary['input_tokens']\n",
    "        \n",
    "        if (token1 and token2) not in input_tokens:\n",
    "            raise ValueError('One or both of the tokens introduced are not in the sentence!')\n",
    "            \n",
    "        idx1, idx2 = input_tokens.index(token1), input_tokens.index(token2)\n",
    "        attention = self.summary['attentions'][:, :, idx1, idx2].numpy()\n",
    "        if display:\n",
    "            TransformerSentence.display_attention(attention, title=(token1, token2))\n",
    "        return attention\n",
    "    \n",
    "    \n",
    "    def attention_from_idx(self, i, j, display=True):\n",
    "        attention = self.summary['attentions'][:, :, i, j].numpy()\n",
    "        if display:\n",
    "            TransformerSentence.display_attention(attention, title=f'Token idx: {(i, j)}')\n",
    "        return attention\n",
    "    \n",
    "    def visualize_token_path(self, fit, \n",
    "                             tokens_to_follow=None, \n",
    "                             print_tokens=False, \n",
    "                             fig_axs=(None, None), \n",
    "                             figsize=(10, 10)):\n",
    "        \n",
    "        if tokens_to_follow is None:\n",
    "            all_tokens = self.summary['input_tokens']\n",
    "            regex = re.compile(r'^[a-zA-Z]')\n",
    "            tokens_to_follow = [i for i in all_tokens if regex.search(i)]\n",
    "            \n",
    "        if print_tokens: print(tokens_to_follow)  \n",
    "            \n",
    "        colors = list(range(len(tokens_to_follow)))\n",
    "        projections = []\n",
    "        layer_depth = self.summary['states'].size()[0]\n",
    "        \n",
    "        for i in range(layer_depth):\n",
    "            layer_embeddings = self.summary['states'][i, :, :]\n",
    "            projection = fit.transform(layer_embeddings)\n",
    "            projections.append(projection)\n",
    "\n",
    "        data = np.stack(projections, axis=0)\n",
    "        if None in fig_axs:\n",
    "            fig, axs = plt.subplots(figsize=figsize)\n",
    "        for token in tokens_to_follow:\n",
    "            i = self.summary['input_tokens'].index(token)\n",
    "            plt.plot(data[:,i,0], data[:,i,1], '-o', alpha=0.3)\n",
    "            plt.annotate(s=token, xy=(data[0, i, 0], data[0, i, 1]))\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def visualize_sentence_shape(self, fit, tokens_to_follow=None, \n",
    "                                 print_tokens=False, \n",
    "                                 fig_axs=(None, None), \n",
    "                                 figsize=(10, 10)):\n",
    "\n",
    "        if tokens_to_follow is None:\n",
    "            all_tokens = self.summary['input_tokens']\n",
    "            regex = re.compile(r'^[a-zA-Z]')\n",
    "            tokens_to_follow = [i for i in all_tokens if regex.search(i)]\n",
    "\n",
    "        if print_tokens: print(tokens_to_follow)  \n",
    "\n",
    "        colors = list(range(len(tokens_to_follow)))\n",
    "        projections = []\n",
    "        layer_depth = self.summary['states'].size()[0]\n",
    "        \n",
    "        # get list of indeces of the tokens to follow\n",
    "        idxs = [self.summary['input_tokens'].index(token) for token in tokens_to_follow] \n",
    "        token_embeddings = self.summary['states'][-1, idxs, :]\n",
    "        data = fit.transform(token_embeddings)\n",
    "        \n",
    "        if None in fig_axs:\n",
    "            fig, axs = plt.subplots(figsize=figsize)\n",
    "            \n",
    "        plt.plot(data[:,0], data[:,1], '-o')\n",
    "        for i, token in enumerate(tokens_to_follow):\n",
    "            plt.annotate(s=token, xy=(data[i, 0], data[i, 1]))\n",
    "        #plt.show()\n",
    "    \n",
    "    \n",
    "    def save(self, name, path='.'):\n",
    "        with open(os.path.join(path, name), 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def visualize_embedding(embedding, title=None, vmax=None, vmin=None):\n",
    "        if (vmax or vmin) is None:\n",
    "            vmax = max(embedding)\n",
    "            vmin = min(embedding)\n",
    "            \n",
    "        N = embedding.size()[0]\n",
    "        h = math.ceil(math.sqrt(N))\n",
    "        # N = a*b where abs(a-b) is minimum\n",
    "        while (N % h != 0):\n",
    "            h -= 1\n",
    "        w = int(N / h)\n",
    "        visualization = embedding.reshape((h, w)).numpy()\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(visualization, vmax=vmax, vmin=vmin, cmap='viridis')\n",
    "        fig.colorbar(im)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def display_attention(attention, title=None):\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(attention, vmin=0., vmax=1., cmap='viridis')\n",
    "        fig.colorbar(im)\n",
    "        if title is not None:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xlabel('HEADS')\n",
    "        ax.set_ylabel('LAYERS')\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def load(name, path='.'):\n",
    "        with open(os.path.join(path, name), 'rb') as file:\n",
    "            SentenceObject = pickle.load(file)\n",
    "        return SentenceObject\n",
    "    \n",
    "    @staticmethod\n",
    "    def make_unique(L):\n",
    "        unique_L = []\n",
    "        for i, v in enumerate(L):\n",
    "            totalcount = L.count(v)\n",
    "            count = L[:i].count(v)\n",
    "            unique_L.append(v + '_' + str(count+1) if totalcount > 1 else v)\n",
    "        return unique_L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models and tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Preloading models (this is the most costly)\n",
    "BertBaseModel = BertModel.from_pretrained('bert-base-uncased')\n",
    "BertBaseTokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "BertLargeModel = BertModel.from_pretrained('bert-large-uncased')\n",
    "BertLargeTokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "SciBertModel = BertModel.from_pretrained('scibert-scivocab-uncased')\n",
    "SciBertTokenizer = BertTokenizer.from_pretrained('scibert-scivocab-uncased')\n",
    "SciBertBaseVocabModel = BertModel.from_pretrained('scibert-basevocab-uncased')\n",
    "SciBertBaseVocabTokenizer = BertTokenizer.from_pretrained('scibert-basevocab-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load quora questions dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# dataset loader function\n",
    "def load_dataset(txt_path=\"../datasets/quora_questions.txt\", \n",
    "                 MODEL=SciBertModel,\n",
    "                 TOKENIZER=SciBertTokenizer):\n",
    "    \n",
    "    # Read input sequences from .txt file and put them in a list\n",
    "    with open(txt_path) as f:\n",
    "        text = f.read()\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
    "    try:\n",
    "        sentences.remove('') # remove possible empty strings\n",
    "    except:\n",
    "        None\n",
    "    \n",
    "    list_SentenceObj, ALL_INITIAL_EMBEDDINGS, ALL_CONTEXT_EMBEDDINGS = [], [], []\n",
    "    \n",
    "    for raw_sentence in tqdm(sentences):\n",
    "        SentenceObj = TransformerSentence(raw_sentence,\n",
    "                                          model=MODEL,\n",
    "                                          tokenizer=TOKENIZER)\n",
    "        SentenceObj.write_summary(print_tokens=False)\n",
    "        list_SentenceObj.append(SentenceObj)\n",
    "        ALL_INITIAL_EMBEDDINGS.append(SentenceObj.summary['states'][0, :, :])\n",
    "        ALL_CONTEXT_EMBEDDINGS.append(SentenceObj.summary['states'][-1, :, :])\n",
    "\n",
    "    ALL_INITIAL_EMBEDDINGS = torch.cat(ALL_INITIAL_EMBEDDINGS, dim=0)\n",
    "    ALL_CONTEXT_EMBEDDINGS = torch.cat(ALL_CONTEXT_EMBEDDINGS, dim=0)\n",
    "    \n",
    "    return list_SentenceObj, ALL_INITIAL_EMBEDDINGS, ALL_CONTEXT_EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 301/301 [00:42<00:00,  7.07it/s]\n",
      "100%|██████████| 301/301 [00:44<00:00,  6.74it/s]\n"
     ]
    }
   ],
   "source": [
    "scibert_sentences, SCI_EMB_i, SCI_EMB_c = load_dataset(MODEL=SciBertModel, \n",
    "                                                       TOKENIZER=SciBertTokenizer)\n",
    "bert_sentences, BASE_EMB_i, BASE_EMB_c = load_dataset(MODEL=BertBaseModel, \n",
    "                                                      TOKENIZER=BertBaseTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Are convolutional neural networks useful for tasks other than image classification?\n",
      "1 Are non-causal temporal convolutions the equivalence of Bi-LSTM?\n",
      "2 Are there any techniques, other than RNN/LSTM, to handle time series data?\n",
      "3 Bayesian Inference: What is a Dirichlet process in layman's terms?\n",
      "4 Can an objective account of statistical inference be based on frequentist methods and Bayesian methods?\n",
      "5 Can Bayesian Network be combined with Deep Learning and/or reinforcement Learning to figure out causation?\n",
      "6 Can deep neural networks learn the minimum function?\n",
      "7 Can facial recognition tools be fooled?\n",
      "8 Can recurrent neural networks with LSTM be used for time series prediction?\n",
      "9 Can RNN be used for time series dynamic modelling?\n",
      "10 If yes, how?\n",
      "11 Can you explain the HMM algorithm?\n",
      "12 Classification (machine learning): When should I use a K-NN classifier over a Naive Bayes classifier?\n",
      "13 Computer Vision: What is the difference between HOG and SIFT feature descriptor?\n",
      "14 Computer Vision: What is the difference between local descriptors and global descriptors?\n",
      "15 Convex Optimization: What's the advantage of alternating direction method of multipliers (ADMM), and what's the use case for this type of method compared against classic gradient descent or conjugate gradient descent method?\n",
      "16 Do most machine learning algorithms run in batch, or do they run every time they get a new bit of data?\n",
      "17 Does computer vision (object detection) work without machine learning?\n",
      "18 Does face recognition technology still recognize a person if they now have wrinkles, scars or had cosmetic surgery done like for example a facelift?\n",
      "19 Does machine learning become more accurate as more data is provided?\n",
      "20 Even though SVM is the best linear classifier, why does it lose to logistic regression in a non-linear case?\n",
      "21 Given data, what steps does one take to choose what distribution (poisson, normal, gamma, beta, etc.) the data should be represented by?\n",
      "22 How are energy based models in deep learning related to probability?\n",
      "23 How are linear regression and gradient descent related, is gradient descent a type of linear regression, and is it similar to ordinary least squares (OLS) and generalized least squares (GLS)?\n",
      "24 How are the Monte Carlo methods used to perform an inference in probabilistic graphical models?\n",
      "25 How are the parameters of a Bayesian network learned?\n",
      "26 How can deep learning benefit from Bayesian methods?\n",
      "27 How can face recognition algorithms avoid being tricked by photos?\n",
      "28 How can I use Gaussian processes to perform regression?\n",
      "29 How could probabilistic computing be implemented?\n",
      "30 How different is a TPU from GPU?\n",
      "31 How do face recognition algorithms detect human faces?\n",
      "32 How do fully convolutional networks upsample their coarse output?\n",
      "33 How do I build a face recognition using OpenCV Library and DLib?\n",
      "34 How do I determine the computational time complexity (Big-O) of back propagation in a feedforward neural network?\n",
      "35 How do I perform feature selection?\n",
      "36 How do I select SVM kernels?\n",
      "37 How do image recognition algorithms work?\n",
      "38 How do we extract CNN deep features?\n",
      "39 How do you determine the value of channels when you design your CNN?\n",
      "40 How do you determine which approach is best when solving a problem, Bayesian or Frequentist?\n",
      "41 How do you find the derivative of the cross-entropy loss function in a convolutional neural network?\n",
      "42 How do you improve the accuracy of a neural network?\n",
      "43 How do you model a class which maximizes event probabilities in the context of predictive modelling?\n",
      "44 How does a conditional probability poisson intersect with exponentials?\n",
      "45 How does Bayesian optimization work?\n",
      "46 How does Bayesian reasoning account for overfitting?\n",
      "47 How does facial recognition deal with hair?\n",
      "48 How does facial recognition distinguish between pictures of a face and the actual face?\n",
      "49 How does fingerprints/facial recognition work?\n",
      "50 How does LSTM help prevent the vanishing (and exploding) gradient problem in a recurrent neural network?\n",
      "51 How does multinomial Naive Bayes work?\n",
      "52 How does OCR and OMR differ?\n",
      "53 How does overfitting happen in a neural network?\n",
      "54 How does the Bayes classifier work?\n",
      "55 How does the face recognition system deal with a new person into the system?\n",
      "56 How does the facial recognition algorithms work (i.e. given an image, find all faces and tag them with names)?\n",
      "57 How does the LDA allocation work?\n",
      "58 What is its working model?\n",
      "59 How does the model Faster R-CNN ResNet 50 work?\n",
      "60 How does the perceptual image hashing work and how do you implement it?\n",
      "61 How does the region proposal network (RPN) in Faster R-CNN work?\n",
      "62 How does the sliding window algorithm work in Faster R-CNN?\n",
      "63 How does word2vec work?\n",
      "64 Can someone walk through a specific example?\n",
      "65 How facial hair and large beards are catered in face recognition algorithm?\n",
      "66 How is a convolutional neural network able to learn invariant features?\n",
      "67 How is graph theory used in data science and neural networks?\n",
      "68 How is isotonic regression used in practice for calibration in machine learning?\n",
      "69 How is LSTM different from RNN?\n",
      "70 In a layman explanation.\n",
      "71 How is the hidden state (h) different from the memory (c) in an LSTM cell?\n",
      "72 How many layers does LSTM have?\n",
      "73 How many parameters are there in \"ResNet-50\"?\n",
      "74 How many terms are required for building a Bayes model?\n",
      "75 How practically relevant are classical machine learning algorithms like SVMs or decision trees as compared to deep learning?\n",
      "76 How would I go about learning how to create a simple color recognition app from scratch (without libraries or other pre-written code)?\n",
      "77 For instance, I want to point my webcam at an object and the name of its color will appear on the screen.\n",
      "78 In layman's terms what are the differences and similarities between Bayes Networks, Markov Decision Process, and Hidden Markov Models?\n",
      "79 In layman's terms, how does Gibbs Sampling work?\n",
      "80 In layman's terms, how does Naive Bayes work?\n",
      "81 In LSTM, how do you figure out what size the weights are supposed to be?\n",
      "82 In Neural Network, each neuron in a hidden layer is said to focus on a certain feature, take eye for example, but how does it deduce what an eye is since the meaning of eye can be a combination of eyebrow and teeth, or some other combinations?\n",
      "83 In optimization, why is Newton's method much faster than gradient descent?\n",
      "84 In Python Keras, how do you subtract the output of two different models and feed it to another model?\n",
      "85 In what situation should I use neural networks instead of machine learning algorithms?\n",
      "86 In what situations is facial recognition not accurate?\n",
      "87 In what types of situations should we employ Vanilla Recurrent Neural Networks instead of LSTM?\n",
      "88 In which cases do GRUs completely outperform LSTMs?\n",
      "89 Intuitively speaking, What is the difference between Bayesian Estimation and Maximum Likelihood Estimation?\n",
      "90 Is Bayesian statistics better or worse than frequentist statistics?\n",
      "91 Is C# a good programming language for doing image processing and computer vision in general?\n",
      "92 Is Convolutional Neural Network basically data-preprocessing via kernel plus Neural Networks?\n",
      "93 Isn't Deep Learning just neural networks with some pre-processing for automated feature selections?\n",
      "94 Is facial recognition accurate?\n",
      "95 Is it easier to build a Neural Machine Translation system than Statistical MT, once you have all the right ingredients?\n",
      "96 Is it possible for a neural network to be too deep?\n",
      "97 Is LSTM a neural network?\n",
      "98 Is the architecture used for text generation in RNNs many-to-many or many-to-one?\n",
      "99 Is the LDA parametric?\n",
      "100 Is there a practical size limitation to convolutional neural networks?\n",
      "101 Is there any correlation between Quine’s underdetermination and Bayesian issues of old evidence and new theories?\n",
      "102 Is wider or deeper convolutional neural networks better for computer vision tasks?\n",
      "103 Many statistical tests in the physical sciences rely on approximations of distribution in order to calculate confidence levels when considering samples from a larger population.\n",
      "104 Should Bayesian methods be used instead?\n",
      "105 Should I go for TensorFlow or PyTorch?\n",
      "106 What algorithms can detect if two images/objects are similar or not?\n",
      "107 What are factorization machines and how are they used in machine learning?\n",
      "108 What are kernels in machine learning and SVM and why do we need them?\n",
      "109 What are Markov Chain Monte Carlo methods in layman's terms?\n",
      "110 What are prior and posterior probabilities?\n",
      "111 What are probabilistic graphical models, and why are they useful?\n",
      "112 What are Siamese neural networks, what applications are they good for, and why?\n",
      "113 What are some applications of Probabilistic Graphical Models?\n",
      "114 What are some benefits and drawbacks of discriminative and generative models?\n",
      "115 What are some fundamental deep learning papers for which code and data is available to reproduce the result and on the way grasp deep learning?\n",
      "116 What are some good papers about topic modeling for short texts (especially for Tweets) ?\n",
      "117 What are some interesting things about machine learning algorithms that few practicing data scientists understand?\n",
      "118 What are some intuitive examples of the Bayes theorem?\n",
      "119 What are some of the current challenges for self-driving/autonomous cars in computer vision context?\n",
      "120 What are some recent advances in non-convex optimization research?\n",
      "121 What are some unsolved problems in deep machine learning?\n",
      "122 What are some very interesting applications of RNNs (including LSTM, GRU, etc.)?\n",
      "123 What are some ways of pre-procesing images before applying convolutional neural networks for the task of image classification?\n",
      "124 What are the advantages of Bayesian methods over frequentist methods in web data?\n",
      "125 What are the advantages of convolution?\n",
      "126 What are the advantages of different classification algorithms?\n",
      "127 What are the advantages of Fully Convolutional Networks over CNNs?\n",
      "128 What are the advantages of logistic regression over decision trees?\n",
      "129 Are there any cases where it's better to use logistic regression instead of decision trees?\n",
      "130 What are the algorithms for determining if a point is inside an arbitrary closed shape or not?\n",
      "131 What are the applications of face recognition?\n",
      "132 What are the benefits of using the Laplace prior over the Gaussian prior in the Bayesian inference?\n",
      "133 What are the best visualizations of machine learning algorithms?\n",
      "134 What are the current major limitations of computer vision?\n",
      "135 What are the differences between DBN and CNN?\n",
      "136 Which one is good for object detection and why?\n",
      "137 What are the differences between generative and discriminative machine learning?\n",
      "138 What are the disadvantages of using a naive bayes for classification?\n",
      "139 What are the interesting differences between the total probability theory and the Bayes theorem?\n",
      "140 What are the limits of deep learning?\n",
      "141 What are the main differences between the word embeddings of ELMo, BERT, Word2vec, and GloVe?\n",
      "142 What are the main drawbacks of current image segmentation algorithms?\n",
      "143 What are the main operational problems of neural networks?\n",
      "144 What are the most critical shortcomings of computer vision when applying models to robots (or automation in general)?\n",
      "145 What are the most important problems in computer vision?\n",
      "146 What are the most important problems in face recognition?\n",
      "147 What are the pros and cons of a Bayesian network versus a Markov decision process?\n",
      "148 What are the pros and cons of using Matlab vs Open CV for image processing?\n",
      "149 What are the real life applications of convex hulls?\n",
      "150 What are the units in LSTM Keras?\n",
      "151 What can cause bad facial recognition?\n",
      "152 What does a 1x1 convolutional layer do?\n",
      "153 What does dimensional reduction mean in a neural network?\n",
      "154 What does it mean if all produced images of a GAN look the same?\n",
      "155 What does max pooling do?\n",
      "156 What does the alpha mean in the dual form of the SVM optimization problem?\n",
      "157 What is 'exact inference' in the context of Probabilistic Graphical Models?\n",
      "158 What is ‘variational inference’ in the context of probabilistic graphical models?\n",
      "159 What is a Bayesian Neural Network?\n",
      "160 What is a convolutional neural network?\n",
      "161 What is a difference between adding one more layer and increasing neurons in one layer?\n",
      "162 What is a good explanation of MIT's new Sparse Fast Fourier Transform for non-technical people?\n",
      "163 What is a layer in TensorFlow?\n",
      "164 What is a neural network accelerator?\n",
      "165 What is a receptive field in a convolutional neural network?\n",
      "166 What is a sequence classification in LSTM?\n",
      "167 What is a simple explanation of how artificial neural networks work?\n",
      "168 What is a stochastic process in layman's terms?\n",
      "169 What is a unit in LSTM?\n",
      "170 What is alpha in multinomial Naive Bayes?\n",
      "171 What is an intuitive explanation for bias-variance tradeoff?\n",
      "172 What is an intuitive explanation for neural networks?\n",
      "173 What is an intuitive explanation of Bayes' Rule?\n",
      "174 What is an intuitive explanation of Convolutional Neural Networks?\n",
      "175 What is an intuitive explanation of Fisher information?\n",
      "176 What is an intuitive explanation of LSTMs and GRUs?\n",
      "177 What is an intuitive explanation of stochastic gradient descent?\n",
      "178 What is an intuitive explanation of the Xavier Initialization for Deep Neural Networks?\n",
      "179 What is Bayes' law?\n",
      "180 What is Bayesian inference in statistics?\n",
      "181 When and how is Bayesian inference used?\n",
      "182 What is better, iris scanning or facial recognition?\n",
      "183 Why?\n",
      "184 What is better, k-nearest neighbors algorithm (k-NN) or Support Vector Machine (SVM) classifier?\n",
      "185 Which algorithm is mostly used practically?\n",
      "186 Which algorithm guarantees reliable detection in unpredictable situations?\n",
      "187 What is causality?\n",
      "188 What is current cutting-edge research in Bayesian statistics (including topics related to machine learning) focused on right now?\n",
      "189 What is deep learning, and what are deep neural networks?\n",
      "190 What is Differentiable Programming?\n",
      "191 What is embedding | embedded space | feature embedding in deep neural architectures?\n",
      "192 What is exactly the attention mechanism introduced to RNN (recurrent neural network)?\n",
      "193 It would be nice if you could make it easy to understand!\n",
      "What is Google's capsule network?\n",
      "194 How is it different than convolutional neural networks?\n",
      "195 To what problems and areas can capsule neural network be applied?\n",
      "196 What is gradient clipping and why is it necessary?\n",
      "197 What is LSTM in Keras?\n",
      "198 What is mirror descent and how is it different from gradient descent?\n",
      "199 What is R-CNN?\n",
      "200 What is SGD in Keras?\n",
      "201 What is Softmax in CNN?\n",
      "202 What is softmax regression and how is it related to logistic regression?\n",
      "203 What is Tesla's 'Deep Rain' and what does it do?\n",
      "204 What is the advantage of combining Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN)?\n",
      "205 What is the best neural network library for Python?\n",
      "206 What is the best OCR software to transform PDF files (with image text) into text files?\n",
      "207 What is the conceptual meaning behind diagonalizing Markov chains?\n",
      "208 What is the core of Bayesian thinking?\n",
      "209 What is the default learning rate in Keras?\n",
      "210 What is the difference between Bayes’ theorem and conditional probability?\n",
      "211 What is the difference between binary classification, Bayes classifier, and Bayes decision boundary?\n",
      "212 What is the difference between CNN and R-CNN?\n",
      "213 What is the difference between detection and classification in computer vision?\n",
      "214 What is the difference between dropout and batch normalization?\n",
      "215 What is the difference between Gaussian and Bayesian?\n",
      "216 What is the difference between hierarchical recurrent neural network and stacked LSTM?\n",
      "217 What is the difference between image processing and computer vision?\n",
      "218 What is the difference between linear convolution and circular convolution?\n",
      "219 What is the difference between logistic regression and Naive Bayes?\n",
      "220 What is the difference between RIPv1 and RIPv2?\n",
      "221 What is the difference between RNN and LSTM?\n",
      "222 What is the difference between SfM and Visual SLAM?\n",
      "223 What is the difference between stacked LSTM's and multidimensional LSTM's?\n",
      "224 What is the difference between the Naive Bayes Classifier and the Bayes classifier?\n",
      "225 What is the difference between topic-word versus document-topic distribution in LDA topical modeling?\n",
      "226 What is the difference between transfer learning and fine tuning?\n",
      "227 What is the formal explanation as to why adversarial model like one pixel attack works against neural network?\n",
      "228 What is the gradient of loss function?\n",
      "229 What is the Hessian matrix?\n",
      "230 What is it used for and for what reason?\n",
      "231 What is the intuition behind Bayesian hierarchical modeling?\n",
      "232 What is the learning rate in the context of deep learning?\n",
      "233 What is the Naive Bayes classifier in machine learning?\n",
      "234 What is the necktie paradox in Bayesian statistics?\n",
      "235 What is the power behind Bayesian networks?\n",
      "236 What is the reason for applying convolutional neural nets to time series instead using recurrent neural nets?\n",
      "237 What is the relation between machine learning, image processing and computer vision?\n",
      "238 What is the relationship between logistic regression and Bayes' theorem?\n",
      "239 What is the relationship between timestep and number hidden unit in LSTM?\n",
      "240 What is the simplest explanation of Bayesian statistics?\n",
      "241 What is the smallest number of data points usable for reliable machine learning?\n",
      "242 What is the TLDR of Spherical Convolutional Neural Networks (ICLR 2018 best paper)?\n",
      "243 What is the vanishing gradient problem?\n",
      "244 What is visual inertial odometry and steps involved in it?\n",
      "245 What machine learning networks/algorithms could be used for images deblurring, without using GANs (too slow to train), within 10 training hours, imperfect deblurring authorized, and deep learning not required (just machine learning)?\n",
      "246 What makes model parameters vs latent variables?\n",
      "247 What newly developed machine learning models could surpass deep learning?\n",
      "248 What shortcomings do you see with deep learning?\n",
      "249 What's a good way to provide intuition as to why the lasso (L1 regularization) results in sparse weight vectors?\n",
      "250 What’s the difference between Bayesian vs.\n",
      "251 frequentist statistical approaches in simple laymen terms?\n",
      "252 How does Bayesian inference overcome limitations of frequentist approaches?\n",
      "253 When building a deep learning model (CNN, LSTM, autoencoder, etc), how do we determine the number of layers, the number of units in each layer, and sometimes the connection between them?\n",
      "254 When do we use Mask R-CNN?\n",
      "255 When does the problem arise of neural networks not converging?\n",
      "256 When should I prefer variational inference over MCMC for Bayesian analysis?\n",
      "257 When should I use an RNN LSTM and when to use ARIMA for a time series forecasting problem?\n",
      "258 What is the relation between them?\n",
      "259 When should you not use random forest?\n",
      "260 Where can I find the best Biometrics facial recognition technology for ID verification?\n",
      "261 Where does each type of neural network (RNN, CNN, LSTM, etc.) excel?\n",
      "262 Where is Sparsity important in Deep Learning?\n",
      "263 Which are the best image editing companies that transform or retouch ordinary images into superior quality images within Adobe Photoshop?\n",
      "264 Which convolutional network is better for time series predictions, Conv1D or Conv2D in a CNN model?\n",
      "265 Which model is more accurate than the combination between LSTM and CNN?\n",
      "266 Which neural network framework is the best, Keras, Torch or Caffe?\n",
      "267 Why are autoencoders considered a failure?\n",
      "268 What are their alternatives?\n",
      "269 Why are CNNs better at classification than RNNs?\n",
      "270 Why are convolutional neural networks better than other neural networks in processing data such as images and video?\n",
      "271 Why are GPUs well-suited to deep learning?\n",
      "272 Why are RNNs, LSTMs used for sentiment analysis?\n",
      "273 Why are Scikit-learn machine learning models not as widely used in industry as TensorFlow or PyTorch?\n",
      "274 Why do artificial neural networks use calculation heavy sigmoid function while it can be approximated efficiently with five segments?\n",
      "275 Why do CNNs work so well with images?\n",
      "276 Why do convex optimization algorithms seem to work well for non-convex problems in machine learning?\n",
      "277 Why do deep learning models use convolutional layers instead of fully-connected layers?\n",
      "278 Why do we need a non-linear activation function in a convolutional neural network?\n",
      "279 The convolution is already non-linear.\n",
      "280 Shouldn't that already be enough?\n",
      "281 Why do we need XGBoost and Random Forest?\n",
      "282 Why do we use Naive Bayes classifiers?\n",
      "283 Why does a convex lens show a bigger image of the actual object?\n",
      "284 Why does batch normalization need moving averages besides to track model accuracy?\n",
      "285 Why does object detection accuracy fail to match superior classification accuracy?\n",
      "286 Why is Bayes' Theorem important?\n",
      "287 Why is collinearity not a problem for logistic regression?\n",
      "288 Why is Gru over LSTM?\n",
      "289 Why is it important to scale your inputs in gradient descent?\n",
      "290 Why is L1 regularization supposed to lead to sparsity than L2?\n",
      "291 Why is logistic regression a classification method instead of a regression method?\n",
      "292 Why is logistic regression more likely to overfit than Naïve Bayes?\n",
      "293 Why is ReLU the most common activation function used in neural networks?\n",
      "294 Why is solving in the dual easier than solving in the primal?\n",
      "295 What advantages do we get from solving in the dual?\n",
      "296 Why is the frequentist approach to machine learning more successful than the Bayesian approach?\n",
      "297 Why is the output of logistic regression interpreted as a probability?\n",
      "298 Why is the pooling layer used in a convolution neural network?\n",
      "299 Why is the pooling layer used in CNN?\n",
      "300 Why would a saturated neuron be a problem?\n"
     ]
    }
   ],
   "source": [
    "for i, sentence in enumerate(bert_sentences):\n",
    "    print(i, sentence.raw_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(TENSORS, LABELS, PATH):\n",
    "    if type(TENSORS) is not np.ndarray:\n",
    "        TENSORS = TENSORS.numpy\n",
    "    \n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "    \n",
    "    np.savetxt(os.path.join(PATH, 'tensors.tsv'), TENSORS, delimiter='\\t', newline='\\n')\n",
    "    with open(os.path.join(PATH, 'labels.tsv'), 'w') as f_output:\n",
    "        tsv_output = csv.writer(f_output, delimiter='\\n')\n",
    "        tsv_output.writerow(LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tensors_and_labels(sentences_list, sentence_indices, layer):\n",
    "    sentences = [sentences_list[i] for i in sentence_indices]\n",
    "    TENSORS, LABELS = [], []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        t = sentence.summary['states'][layer, :, :].numpy()\n",
    "        L = sentence.summary['input_tokens']\n",
    "        idx = sentence_indices[i]\n",
    "        L = [f'{Li} \\t {idx:03d}' for Li in L ]\n",
    "        TENSORS.append(t)\n",
    "        LABELS.extend(L)\n",
    "    \n",
    "    LABELS.insert(0, f'token \\t sentence_idx')    \n",
    "    TENSORS = np.concatenate(TENSORS, axis=0)\n",
    "    \n",
    "    return TENSORS, LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings from sentences and save them as .tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [7, 35, 97, 118]\n",
    "sentences = [scibert_sentences[i] for i in indices]\n",
    "PATH = '/Users/sergicastellasape/Repos/zeta-alpha/data/embeddings/scibert/contextual-7-35-97-118'\n",
    "TENSORS, LABELS = get_tensors_and_labels(scibert_sentences, indices, -1)\n",
    "save_data(TENSORS, LABELS, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Zeta Alpha",
   "language": "python",
   "name": "za_conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
